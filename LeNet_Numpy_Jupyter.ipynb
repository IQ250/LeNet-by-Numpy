{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import logging\n",
    "import time\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating log file\n",
    "def create_logger(output_path, cfg_name):\n",
    "    log_file = '{}_{}.log'.format(cfg_name, time.strftime('%Y-%m-%d-%H-%M'))\n",
    "    head = '%(asctime)-15s %(message)s'\n",
    "    logging.basicConfig(filename=os.path.join(output_path, log_file), format=head)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "# Loading and precessing data\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, '%s-labels.idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images.idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parts of Model\n",
    "def initialize_parameters(layers):\n",
    "    \"\"\"\n",
    "    Initialize parameters according to different types of layers\n",
    "    \n",
    "    Argument:\n",
    "    layers -- list, the length denotes the depth of networks, every element is a dictionary which contains the \n",
    "              mode of layer and shape of weight and bias \n",
    "    \n",
    "    Returns:\n",
    "    new_layers -- list, every element corresponds to original layer and its intialized parameter\n",
    "    \"\"\"\n",
    "    new_layers = []\n",
    "    for i, layer in enumerate(layers):\n",
    "        mode = layer['mode'] # 'fc', 'conv', 'pool'\n",
    "        if mode == 'pool':\n",
    "            new_layers.append(layer)\n",
    "            continue\n",
    "        elif mode == 'fc':\n",
    "            n_now = layer['n_now']\n",
    "            n_prev = layer['n_prev']\n",
    "            layer['W']=(np.random.rand(n_now, n_prev) - 0.5) * 0.2 # random sample in [-0.1, 0.1] \n",
    "            layer['b']=(np.random.rand(n_now,1) - 0.5) * 0.2\n",
    "            layer['dW']=np.zeros_like(layer['W'])\n",
    "            layer['db']=np.zeros_like(layer['b'])\n",
    "        elif mode == 'conv':\n",
    "            f = layer['f']\n",
    "            n_C = layer['n_C']\n",
    "            n_C_prev = layer['n_C_prev']\n",
    "            layer['W']=(np.random.rand(f, f, n_C_prev, n_C) - 0.5) * 0.2 # random sample in [-0.1, 0.1] \n",
    "            layer['b']=(np.random.rand(1, 1, 1, n_C) - 0.5) * 0.2\n",
    "            layer['dW']=np.zeros_like(layer['W'])\n",
    "            layer['db']=np.zeros_like(layer['b'])\n",
    "        else:\n",
    "            print('Wrong layer in [{}]'.format(i))\n",
    "        new_layers.append(layer)\n",
    "            \n",
    "    return new_layers\n",
    "\n",
    "def sigmoid(Z):\n",
    "    # Sigmoid activation function\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    # Backpropogation of sigmoid activation function\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    return dZ\n",
    "\n",
    "def relu(Z):\n",
    "    # Relu activation function\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    # Backpropogation of Relu activation function \n",
    "    Z = cache\n",
    "    \n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    dZ[Z < 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def softmax(Z):\n",
    "    # Softmax activation function\n",
    "    n, m = Z.shape\n",
    "    A = np.exp(Z)\n",
    "    A_sum = np.sum(A, axis = 0)\n",
    "    A_sum = A_sum.reshape(-1, m)\n",
    "    A = A / A_sum\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax_backward(A, Y):\n",
    "    # Backpropogation of softmax activation function\n",
    "    # loss = - ln a[j] (y[j] = 1, j = {0, ..., n}) \n",
    "    m = A.shape[1]\n",
    "    dZ = (A - Y) / np.float(m)\n",
    "    return dZ\n",
    "\n",
    "def linear_activation_forward(A_prev, layer, activation='relu'):\n",
    "    W = layer['W']\n",
    "    b = layer['b']\n",
    "    if activation=='sigmoid':\n",
    "        Z, linear_cache=np.dot(W, A_prev)+b, (A_prev, W, b)\n",
    "        A, activation_cache=sigmoid(Z)\n",
    "    elif activation=='relu':\n",
    "        Z, linear_cache=np.dot(W, A_prev)+b, (A_prev, W, b)\n",
    "        A, activation_cache=relu(Z)\n",
    "    else:\n",
    "        Z = np.dot(W, A_prev)+b\n",
    "        A = Z\n",
    "    return A, Z\n",
    "\n",
    "def linear_activation_backward(dA, layer, activation):\n",
    "    # Backward propagatIon module - linear activation backward\n",
    "    A_prev = layer['A_prev']\n",
    "    W = layer['W']\n",
    "    b = layer['b']\n",
    "    Z = layer['Z']\n",
    "    if activation=='relu':\n",
    "        dZ=relu_backward(dA, Z)\n",
    "    elif activation=='sigmoid':\n",
    "        dZ=sigmoid_backward(dA, Z)\n",
    "    else:\n",
    "        dZ = dA \n",
    "    n, m = dA.shape\n",
    "    dA_prev=np.dot(W.T, dZ)\n",
    "    dW = np.dot(dZ, A_prev.T)\n",
    "    db = np.sum(dZ, axis = 1).reshape(n,1)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def zero_pad(X, pad, value = 0):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    X_pad = np.pad(X, ((0, 0),(pad, pad),(pad, pad),(0, 0)), 'constant', constant_values=value)\n",
    "    \n",
    "    return X_pad\n",
    "\n",
    "def conv_forward(A_prev, layer):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    layer -- a dictionary contains weights, bias, hyperparameters and shape of data\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    # Retrieve information from layer\n",
    "    W = layer['W']\n",
    "    b = layer['b']\n",
    "    stride = layer['s']\n",
    "    pad = layer['p']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor\n",
    "    n_H = 1 + int((n_H_prev + 2 * pad - f) / stride)\n",
    "    n_W = 1 + int((n_W_prev + 2 * pad - f) / stride)\n",
    "    \n",
    "    # Initialize the output volume Z with zeros\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    if pad > 0:\n",
    "        A_prev_pad = zero_pad(A_prev, pad)\n",
    "    else:\n",
    "        A_prev_pad = A_prev\n",
    "    \n",
    "    for i in range(m):                                 # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i]                     # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (â‰ˆ4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                  \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron\n",
    "                    Z[i, h, w, c] = np.sum(np.multiply(a_slice_prev, W[:, :, :, c])) + b[0, 0, 0, c]\n",
    "\n",
    "    return Z\n",
    "\n",
    "def conv_backward(dZ, layer):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    layer -- a dictionary contains weights, bias, hyperparameters and shape of data\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    # Retrieve informations from layer\n",
    "    A_prev = layer['A_prev']\n",
    "    W = layer['W']\n",
    "    b = layer['b']\n",
    "    Z = layer['Z']\n",
    "    stride = layer['s']\n",
    "    pad = layer['p']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    if pad > 0:\n",
    "        A_prev_pad = zero_pad(A_prev, pad)\n",
    "        dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    else:\n",
    "        A_prev_pad = A_prev\n",
    "        dA_prev_pad = np.copy(dA_prev)\n",
    "\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = A_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas\n",
    "                    dA_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += dZ[i, h, w, c] * a_slice\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad\n",
    "        if pad == 0:\n",
    "            dA_prev[i, :, :, :] = dA_prev_pad[i, :, :, :]\n",
    "        else:\n",
    "            dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def pool_forward(A_prev, layer, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = layer[\"f\"]\n",
    "    stride = layer[\"s\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "\n",
    "    for i in range(m):                           # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c.\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "\n",
    "    return A\n",
    "\n",
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    # Retrieve dimensions from shape\n",
    "    (n_H, n_W) = shape\n",
    "    # Compute the value to distribute on the matrix\n",
    "    average = dz / (n_H * n_W)\n",
    "    # Create a matrix where every entry is the \"average\" value\n",
    "    a = np.ones(shape) * average\n",
    "    \n",
    "    return a\n",
    "\n",
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    mask = (x == np.max(x))\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def pool_backward(dA, layer, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    # Retrieve information from layer\n",
    "    A_prev = layer['A_prev']\n",
    "    stride = layer['s']\n",
    "    f = layer['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        # select training example from A_prev\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    # Find the corners of the current \"slice\" \n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i, h, w, c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        # Get the value a from dA\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da.\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "\n",
    "    return dA_prev\n",
    "\n",
    "def forward_propogation(X, layers):\n",
    "    m = X.shape[0]\n",
    "    # -1- convolution layer\n",
    "    layers[0]['A_prev'] = X\n",
    "    Z = conv_forward(X, layers[0])\n",
    "    layers[0]['Z'] = Z\n",
    "    A, _ = relu(Z)\n",
    "    \n",
    "    # -2- average pooling layer\n",
    "    layers[1]['A_prev'] = A\n",
    "    A = pool_forward(A, layers[1], mode = \"average\")\n",
    "    \n",
    "    # -3- convolution layer\n",
    "    layers[2]['A_prev'] = A\n",
    "    Z = conv_forward(A, layers[2])\n",
    "    layers[2]['Z'] = Z\n",
    "    A, _ = relu(Z)\n",
    "    \n",
    "    # -4- average pooling layer\n",
    "    layers[3]['A_prev'] = A\n",
    "    A = pool_forward(A, layers[3], mode = \"average\")\n",
    "    \n",
    "    # -5- convolution layer\n",
    "    layers[4]['A_prev'] = A\n",
    "    Z = conv_forward(A, layers[4])\n",
    "    layers[4]['Z'] = Z\n",
    "    A, _ = relu(Z)\n",
    "    \n",
    "    # -6- fully connected layer\n",
    "    layers[5]['A_prev'] = (A.reshape(m,-1)).T # flatten\n",
    "    A, Z = linear_activation_forward((A.reshape(m,-1)).T, layers[5], activation='relu')\n",
    "    layers[5]['Z'] = Z\n",
    "    \n",
    "    # -7- fully connected layer\n",
    "    layers[6]['A_prev'] = A\n",
    "    _, Z = linear_activation_forward(A, layers[6], activation='none')\n",
    "    layers[6]['Z'] = Z\n",
    "    AL, _ = softmax(Z)\n",
    "\n",
    "    return AL, layers\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    n, m = Y.shape\n",
    "    cost = - np.sum(np.log(AL) * Y) / m\n",
    "    cost=np.squeeze(cost)\n",
    "\n",
    "    return cost\n",
    "\n",
    "def backward_propogation(AL, Y, layers):\n",
    "    m = Y.shape[1]\n",
    "    # -7- fully connected layer\n",
    "    dZ = softmax_backward(AL, Y)\n",
    "    dA_prev, dW, db = linear_activation_backward(dZ, layers[6], 'none')\n",
    "    layers[6]['dW'] = dW\n",
    "    layers[6]['db'] = db\n",
    "    \n",
    "    # -6- fully connected layer\n",
    "    dA_prev, dW, db = linear_activation_backward(dA_prev, layers[5], 'relu')\n",
    "    layers[5]['dW'] = dW\n",
    "    layers[5]['db'] = db\n",
    "    \n",
    "    # -5- convolution layer\n",
    "    dA = (dA_prev.T).reshape(m,1,1,layers[4]['n_C']) # flatten backward\n",
    "    dZ = relu_backward(dA, layers[4]['Z'])\n",
    "    dA_prev, dW, db = conv_backward(dZ, layers[4])\n",
    "    layers[4]['dW'] = dW\n",
    "    layers[4]['db'] = db\n",
    "    \n",
    "    # -4- average pooling layer\n",
    "    dA_prev = pool_backward(dA_prev, layers[3], mode = \"average\")\n",
    "    \n",
    "    # -3- convolution layer\n",
    "    dZ = relu_backward(dA_prev, layers[2]['Z'])\n",
    "    dA_prev, dW, db = conv_backward(dZ, layers[2])\n",
    "    layers[2]['dW'] = dW\n",
    "    layers[2]['db'] = db\n",
    "    \n",
    "    # -2- average pooling layer\n",
    "    dA_prev = pool_backward(dA_prev, layers[1], mode = \"average\")\n",
    "    \n",
    "    # -1- convolution layer\n",
    "    dZ = relu_backward(dA_prev, layers[0]['Z'])\n",
    "    dA_prev, dW, db = conv_backward(dZ, layers[0])\n",
    "    layers[0]['dW'] = dW\n",
    "    layers[0]['db'] = db\n",
    "    \n",
    "    return layers\n",
    "\n",
    "def update_parameters(layers, learning_rate):\n",
    "    num_layer = len(layers)\n",
    "    for i in range(num_layer):\n",
    "        mode = layers[i]['mode'] # 'fc', 'conv', 'pool'\n",
    "        if mode == 'pool':\n",
    "            continue\n",
    "        elif (mode == 'fc' or mode == 'conv'):\n",
    "            layers[i]['W'] = layers[i]['W'] - learning_rate*layers[i]['dW']\n",
    "            layers[i]['b'] = layers[i]['b'] - learning_rate*layers[i]['db']\n",
    "        else:\n",
    "            print('Wrong layer mode in [{}]'.format(i))\n",
    "\n",
    "    return layers\n",
    "\n",
    "def predict(X_test, Y_test, layers):\n",
    "    m = X_test.shape[0]\n",
    "    n = Y_test.shape[1]\n",
    "    pred = np.zeros((n,m))\n",
    "    pred_count = np.zeros((n,m)) - 1 # for counting accurate predictions \n",
    "    \n",
    "    # Forward propagation\n",
    "    AL, _ = forward_propogation(X_test, layers)\n",
    "\n",
    "    # convert prediction to 0/1 form\n",
    "    max_index = np.argmax(AL, axis = 0)\n",
    "    pred[max_index, list(range(m))] = 1\n",
    "    pred_count[max_index, list(range(m))] = 1\n",
    "    \n",
    "    accuracy = np.float(np.sum(pred_count == Y_test.T)) / m\n",
    "    \n",
    "    return pred, accuracy\n",
    "\n",
    "def compute_accuracy(AL, Y):\n",
    "    n, m = Y.shape\n",
    "    pred_count = np.zeros((n,m)) - 1\n",
    "    \n",
    "    max_index = np.argmax(AL, axis = 0)\n",
    "    pred_count[max_index, list(range(m))] = 1\n",
    "    \n",
    "    accuracy = np.float(np.sum(pred_count == Y)) / m\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def train_mini_batch(X_train, Y_train, X_test, Y_test, layers, logger, num_exp=0, batch_size=10, num_epoch=1, learning_rate=0.01):\n",
    "    logger.info('------------ Integer order CNN with mini batch ------------')\n",
    "    logger.info('Initial weights: FC [-0.1, 0.1], CONV [-0.1, 0.1]')\n",
    "    logger.info('Initial bias: FC [-0.1, 0.1], CONV [-0.1, 0.1]')\n",
    "    logger.info('Batch size: {}'.format(batch_size))\n",
    "    logger.info('Learning rate: {}'.format(learning_rate))\n",
    "    \n",
    "    # number of iteration\n",
    "    num_sample=X_train.shape[0]\n",
    "    num_iteration = num_sample // batch_size\n",
    "    index = list(range(num_sample))\n",
    "    \n",
    "    accuracy_train_list = []\n",
    "    accuracy_test_list = []\n",
    "    for epoch in range(num_epoch):\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        random.seed(num_exp*10+epoch)\n",
    "        random.shuffle(index) # random sampling every epoch\n",
    "        for iteration in range(num_iteration):\n",
    "            batch_start = iteration * batch_size\n",
    "            batch_end = (iteration + 1) * batch_size\n",
    "            if batch_end > num_sample:\n",
    "                batch_end = num_sample\n",
    "            X_train_batch = X_train[index[batch_start:batch_end]]\n",
    "            Y_train_batch = Y_train[index[batch_start:batch_end]]\n",
    "            AL, layers = forward_propogation(X_train_batch, layers)\n",
    "            loss = compute_cost(AL, Y_train_batch.T)\n",
    "            accuracy = compute_accuracy(AL, Y_train_batch.T)\n",
    "            layers = backward_propogation(AL, Y_train_batch.T, layers)\n",
    "            layers = update_parameters(layers, learning_rate)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(accuracy)\n",
    "            if (iteration+1) % 600 == 0:\n",
    "                logger.info('Epoch [{}] Iteration [{}]: loss = {} accuracy = {}'.format(epoch, iteration+1, loss, accuracy))\n",
    "                print('Epoch [{}] Iteration [{}]: loss = {} accuracy = {}'.format(epoch, iteration+1, loss, accuracy))\n",
    "                np.save('data/layers_{}_{}.npy'.format(epoch, iteration+1), layers)\n",
    "\n",
    "        _, accuracy_test = predict(X_test, Y_test, layers)\n",
    "        pred_train, _ = forward_propogation(X_train[:10000], layers)\n",
    "        loss_train = compute_cost(pred_train, Y_train[:10000].T)\n",
    "        accuracy_train = compute_accuracy(pred_train, Y_train[:10000].T)\n",
    "        accuracy_train_list.append(accuracy_train)\n",
    "        accuracy_test_list.append(accuracy_test)\n",
    "        print('Epoch [{}] average_loss = {} average_accuracy = {}'.format(epoch, np.mean(losses), np.mean(accuracies)))\n",
    "        logger.info('Epoch [{}] average_loss = {} average_accuracy = {}'.format(epoch, np.mean(losses), np.mean(accuracies)))\n",
    "        print('Epoch [{}] train_loss = {} train_accuracy = {}'.format(epoch, loss_train, accuracy_train))\n",
    "        logger.info('Epoch [{}] train_loss = {} train_accuracy = {}'.format(epoch, loss_train, accuracy_train))\n",
    "        print('Epoch [{}] test_accuracy = {}'.format(epoch, accuracy_test))\n",
    "        logger.info('Epoch [{}] test_accuracy = {}'.format(epoch, accuracy_test))\n",
    "    \n",
    "    return layers, accuracy_train_list, accuracy_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create log file\n",
    "logger = create_logger('output', 'train_log')\n",
    "\n",
    "# Load dataset and reshape image set as (m, n_H, n_W, n_C)\n",
    "X_train, Y_train = load_mnist('data', 'train')\n",
    "X_test, Y_test = load_mnist('data', 'test')\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_test = X_test.reshape(-1,28,28,1)\n",
    "\n",
    "# Normalization for images\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Transform the label into one-hot form\n",
    "(num_train,) = Y_train.shape\n",
    "Y = np.zeros((num_train, 10))\n",
    "for i in range(num_train):\n",
    "    Y[i, Y_train[i]] = 1\n",
    "Y_train = Y\n",
    "(num_test,) = Y_test.shape\n",
    "Y = np.zeros((num_test, 10))\n",
    "for i in range(num_test):\n",
    "    Y[i, Y_test[i]] = 1\n",
    "Y_test = Y\n",
    "\n",
    "# Construct model\n",
    "layer1={}\n",
    "layer1['mode'] = 'conv'\n",
    "layer1['f'] = 5\n",
    "layer1['n_C_prev'] = 1\n",
    "layer1['n_C'] = 6\n",
    "layer1['p'] = 2\n",
    "layer1['s'] = 1\n",
    "layer2={}\n",
    "layer2['mode'] = 'pool'\n",
    "layer2['f'] = 2\n",
    "layer2['s'] = 2\n",
    "layer3={}\n",
    "layer3['mode'] = 'conv'\n",
    "layer3['f'] = 5\n",
    "layer3['n_C_prev'] = 6\n",
    "layer3['n_C'] = 16\n",
    "layer3['p'] = 0\n",
    "layer3['s'] = 1\n",
    "layer4={}\n",
    "layer4['mode'] = 'pool'\n",
    "layer4['f'] = 2\n",
    "layer4['s'] = 2\n",
    "layer5={}\n",
    "layer5['mode'] = 'conv'\n",
    "layer5['f'] = 5\n",
    "layer5['n_C_prev'] = 16\n",
    "layer5['n_C'] = 120\n",
    "layer5['p'] = 0\n",
    "layer5['s'] = 1\n",
    "layer6={}\n",
    "layer6['mode'] = 'fc'\n",
    "layer6['n_now'] = 84\n",
    "layer6['n_prev'] = 120\n",
    "layer7={}\n",
    "layer7['mode'] = 'fc'\n",
    "layer7['n_now'] = 10\n",
    "layer7['n_prev'] = 84\n",
    "construct_layers = [layer1, layer2, layer3, layer4, layer5, layer6, layer7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_experiments = 1\n",
    "for index in range(num_experiments):\n",
    "    print('------------------------------------- Experiment {} -------------------------------------'.format(index+1))\n",
    "    logger.info('------------------------------------- Experiment {} -------------------------------------'.format(index+1))\n",
    "\n",
    "    initial_layers_path = 'data/initial_layers_{}.npy'.format(index+1)\n",
    "    if os.path.exists(initial_layers_path):\n",
    "        initial_layers = np.load(initial_layers_path)\n",
    "        print('Load initial parameters from {}'.format(initial_layers_path))\n",
    "        logger.info('Load initial parameters from {}'.format(initial_layers_path))\n",
    "    else:\n",
    "        initial_layers = initialize_parameters(construct_layers)\n",
    "        np.save(initial_layers_path, initial_layers)\n",
    "        print('Initialize layers and save as {}'.format(initial_layers_path))\n",
    "        logger.info('Initialize layers and save as {}'.format(initial_layers_path))\n",
    "    \n",
    "    print('----------------------------------------------------------------------------------------')\n",
    "    logger.info('----------------------------------------------------------------------------------------')\n",
    "    layers, train_acc, test_acc = train_mini_batch(X_train, Y_train, X_test, Y_test, initial_layers,\n",
    "                                    logger, num_exp=index, batch_size=10, num_epoch=1, learning_rate=0.1)\n",
    "    print('\\n')\n",
    "    logger.info('\\n')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
